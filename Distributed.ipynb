{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from lib import Model, Dist, WeightGrad\n",
    "from drawing import draw, draw_group\n",
    "from chalk import vcat\n",
    "import asyncio\n",
    "import chalk\n",
    "chalk.set_svg_height(400)\n",
    "chalk.set_svg_draw_height(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6355a",
   "metadata": {},
   "source": [
    "# Distributed Training Puzzles\n",
    "\n",
    "by Sasha Rush (@srush_nlp)\n",
    "\n",
    "These puzzles are based off of the paper https://arxiv.org/abs/2211.05953b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2918f",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "The goal of these puzzles is to learn about distributed training of LLMs. However, we will be primarily concerned with a speed and memory efficiency of completing a single update of the models. To make things simpler, we will abstract away from the standard tensor-based transformer model, and just consider a state-less representation of each of the components of a multi-layer neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfa431",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers=2, batches=4)\n",
    "weights, opt_states, activations, grad_activations, grad_weights = model.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c6c14",
   "metadata": {},
   "source": [
    "Our library has 5 parts: \n",
    "\n",
    "* Weights\n",
    "* Optimizer States - Values needed to update the weights\n",
    "* Activations - The internal values computed on the forward pass\n",
    "* Grad Activations - The gradients of the loss wrt to activations, needed for backward pass\n",
    "* Grad Weights - The gradients of the loss wrt to weights, needed for updates\n",
    "\n",
    "For these puzzles, you are *not allowed* to have local variables. You need to store each of these in the dictionary corresponding to its type.\n",
    "\n",
    "We begin by tracing the lifecycle of a single model update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input activations to the model for batches 2, 3 \n",
    "activations[0] = model.get_activation(batches=[2, 3])\n",
    "activations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights (random) for layers 0 and 1\n",
    "for i in range(model.LAYERS):\n",
    "    weights[i], opt_states[i] = model.load_weights(i)\n",
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08760a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations can be moved forward a layer if you have the weights.\n",
    "activations[1] = model.forward(layer=0, inp=activations[0], weight=weights[0])\n",
    "activations[2] = model.forward(layer=1, inp=activations[1], weight=weights[1])\n",
    "activations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97016305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw all the current activations in memory.\n",
    "draw_group(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the last layer, we can convert an activation to a grad activation by calling `loss`\n",
    "grad_activations[model.LAYERS] = model.loss(activations[model.LAYERS])\n",
    "grad_activations[model.LAYERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af45e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `backward` requires the forward activation, the backward grad activation, and the weights.\n",
    "# It returns the grad weights and the backward activation.\n",
    "grad_weights[1], grad_activations[1] = model.backward(1, activations[1], grad_activations[2], weights[1])\n",
    "grad_weights[0], grad_activations[0] = model.backward(0, activations[0], grad_activations[1], weights[0])\n",
    "grad_activations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fac31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use delete to remove any memory that is not longer needed. \n",
    "print(\"Before memory:\", model.memory())\n",
    "del grad_activations[1]\n",
    "print(\"After memory:\", model.memory())\n",
    "\n",
    "draw_group(grad_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04537c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad weights keep track of which batches they are for. Here we only have the grad weights for batches 2 and 3.\n",
    "draw_group(grad_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we try to update with the grad weights we will get an error.\n",
    "try:\n",
    "    model.update(0, weight_grad=grad_weights[0], weight=weights[0], opt_state=opt_states[0])\n",
    "except AssertionError as e:\n",
    "    print(\"Error! Only have batches\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba70c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# For this example, we can cheat. Pretend we had the other gradients we needed. \n",
    "grad_weights[0, 0] = model.fake_grad(0, [0,1])\n",
    "grad_weights[1, 0] = model.fake_grad(1, [0,1])\n",
    "grad_weights[0, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb746e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing together grad_weights gives the full gradient.\n",
    "grad_weights[0] = grad_weights[0] + grad_weights[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94c2d0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Now we can call update to the get the new weights and opt_state.\n",
    "weights[0], opt_states[0] = model.update(0, weight_grad=grad_weights[0], weight=weights[0], \n",
    "                                         opt_state=opt_states[0])\n",
    "weights[1], opt_states[1] = model.update(1, weight_grad=grad_weights[1] + grad_weights[1, 0], \n",
    "                                         weight=weights[1], opt_state=opt_states[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9843a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can complete the tests by setting these as the final weights and calling check.\n",
    "model.set_final_weight(0, weights[0])\n",
    "model.set_final_weight(1, weights[1])\n",
    "model.check([model])\n",
    "draw_group(model.final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the final outcome of the system as a diagram. \n",
    "# This show the forward and backward passes (numbers of batches) and the updates.\n",
    "# The lines on the bottom show the memory that is used at each time step.\n",
    "draw([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e163f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cd32387",
   "metadata": {},
   "source": [
    "### Puzzle 0 - Standard Training\n",
    "\n",
    "Write a standard (non-distributed) training loop that acts on all the batches and loads all the weights. It should just run forward, loss, backward, and update. Aim for the least amount of max memory used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(model: Model) -> Model:\n",
    "    # Storage on device.\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "\n",
    "    # Load in the full weights\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.load_weights(l)\n",
    "\n",
    "    # Load the input layer activations\n",
    "    activations[0] = model.get_activation(range(model.BATCHES))\n",
    "\n",
    "    ## USER CODE\n",
    "    # Forward\n",
    "    for l in range(model.LAYERS):\n",
    "        activations[l + 1] = model.forward(l, activations[l], weights[l])\n",
    "\n",
    "    # Backward\n",
    "    grad_activations[model.LAYERS] = model.loss(activations[model.LAYERS])\n",
    "    del activations[model.LAYERS]\n",
    "    \n",
    "    for l in range(model.LAYERS - 1, -1, -1):\n",
    "        grad_weights[l], grad_activations[l] = model.backward(\n",
    "            l, activations[l], grad_activations[l + 1], weights[l]\n",
    "        )\n",
    "        del grad_activations[l + 1], activations[l]\n",
    "    del grad_activations[0]\n",
    "    assert len(grad_activations) == 0 and len(activations) ==0\n",
    "\n",
    "    # Update\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.update(l, grad_weights[l], weights[l], opt_states[l])\n",
    "    ## END USER CODE\n",
    "    \n",
    "    for l in range(model.LAYERS):\n",
    "        model.set_final_weight(l, weights[l])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = basic(Model(layers=2, batches=4, rank=0, dist=Dist(1)))\n",
    "draw_group(out.final_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw([out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.check([out])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20cb471",
   "metadata": {},
   "source": [
    "### Puzzle 1 - Gradient Accumulation\n",
    "\n",
    "For this puzzle, the goal is to reduce max memory usage. To do so you are going to run on each batch individually instead of all together. \n",
    "\n",
    "Write a function with four parts. First run on batches {0} and then {1} etc. Sum the grad weights and then update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_accum(model: Model) -> Model:\n",
    "    # Storage on device.\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "\n",
    "    # Load in the full weights\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.load_weights(l)\n",
    "\n",
    "    for r in range(model.BATCHES):\n",
    "        # Load the input layer activations\n",
    "        activations[0, r] = model.get_activation([r])\n",
    "\n",
    "        ## USER CODE\n",
    "        # Forward\n",
    "        for l in range(model.LAYERS):\n",
    "            activations[l + 1, r] = model.forward(l, activations[l, r], weights[l])\n",
    "\n",
    "        # Backward\n",
    "        grad_activations[model.LAYERS, r] = model.loss(activations[model.LAYERS, r])\n",
    "        del activations[model.LAYERS, r]\n",
    "        \n",
    "        for l in range(model.LAYERS - 1, -1, -1):\n",
    "            grad_weights[l, r], grad_activations[l, r] = model.backward(\n",
    "                l, activations[l, r], grad_activations[l + 1, r], weights[l]\n",
    "            )\n",
    "            del grad_activations[l + 1, r], activations[l,r]\n",
    "        del grad_activations[0, r]\n",
    "        assert len(grad_activations) == 0 and len(activations) == 0\n",
    "\n",
    "    # Update\n",
    "    for l in range(model.LAYERS):\n",
    "        for r in range(1, model.BATCHES):\n",
    "            grad_weights[l, 0] = grad_weights[l, 0] + grad_weights[l, r]\n",
    "        weights[l], opt_states[l] = \\\n",
    "            model.update(l, \n",
    "                        grad_weights[l, 0], weights[l], opt_states[l])\n",
    "\n",
    "    ## User Code\n",
    "    for l in range(model.LAYERS):\n",
    "        model.set_final_weight(l, weights[l])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = grad_accum(Model(layers=2, batches=4, rank=0, dist=Dist(1)))\n",
    "draw_group(out.final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dfd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw([out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check([out])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680bec80",
   "metadata": {},
   "source": [
    "## Communications: AllReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a704f13",
   "metadata": {},
   "source": [
    "When working with multiple GPUs we need to have communication. \n",
    "The primary communication primitives for GPUs are implemented in NCCL. \n",
    "\n",
    "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html\n",
    "\n",
    "We are not going to use these directly, but simulate them using Python and asyncio. \n",
    "\n",
    "The first operation is AllReduce. We will have 4 GPUs (ranks=4) and use them each to compute a batch of weight grads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa219d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = 4\n",
    "weight_grads = [WeightGrad(0, 1, {i}, ranks) for i in range(ranks)]\n",
    "weight_grads[0] + weight_grads[1] + weight_grads[2] + weight_grads[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306fbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple asynchronous function that calls allreduce to sum the weight grads at layer 0\n",
    "async def myfunc(model: Model) -> WeightGrad:\n",
    "    return await model.allreduce(weight_grads[model.rank], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fe4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses asyncio to run the above function on 4 \"GPUs\" .\n",
    "dist = Dist(ranks)\n",
    "out_weight_grads = await asyncio.gather(*[\n",
    "    myfunc(Model(layers=1, batches=1, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "out_weight_grads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b82fe3",
   "metadata": {},
   "source": [
    "### Puzzle 2 - Distributed Data Parallel\n",
    "\n",
    "Write a function with four parts. First run on batches {0} and then {1} etc. Sum the grad weights and then update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6d401",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "async def ddp(model: Model) -> Model:\n",
    "    # Storage on device.\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "    # Load all the activations\n",
    "    model.activations[0] = model.get_activation([model.rank])\n",
    "\n",
    "    # Load in the full weights\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.load_weights(l)\n",
    "\n",
    "    # Forward\n",
    "    for l in range(model.LAYERS):\n",
    "        activations[l + 1] = model.forward(l, activations[l], weights[l])\n",
    "\n",
    "    # Backward\n",
    "    grad_activations[model.LAYERS] = model.loss(activations[model.LAYERS])\n",
    "\n",
    "    for l in range(model.LAYERS - 1, -1, -1):\n",
    "        grad_weights[l], grad_activations[l] = model.backward(\n",
    "            l, activations[l], grad_activations[l + 1], weights[l]\n",
    "        )\n",
    "        del grad_activations[l + 1], activations[l]\n",
    "\n",
    "    # Update\n",
    "    for l in range(model.LAYERS):\n",
    "        grad_weights[l] = await model.allreduce(grad_weights[l], l)\n",
    "        weights[l], opt_states[l] = model.update(l, grad_weights[l], weights[l], opt_states[l])\n",
    "        model.set_final_weight(l, weights[l])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac62a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Dist(ranks)\n",
    "out = await asyncio.gather(*[\n",
    "    ddp(Model(layers=2, batches=ranks, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "draw_group(out[0].final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176e3d5",
   "metadata": {},
   "source": [
    "## Communication: AllGather / Sharding\n",
    "\n",
    "Our next primitive is AllGather. This allows us to communicate \"shards\" of an object stored on different GPUs to all the GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685eef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only part of a weights.\n",
    "model = Model(layers=2, batches=1, rank=0, dist=Dist(1))\n",
    "weight, _ = model.load_weights(0, shard=0, total=4)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411af1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine togegher two shards on one machine.\n",
    "weights = [model.load_weights(0, shard=i, total=ranks)[0] for i in range(ranks)]\n",
    "weights[0].combine(weights[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48625837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use allgather to collect the shards from all machines.\n",
    "async def mygather(model: Model) -> WeightGrad:\n",
    "    # Allreduce sums together all the weight grads\n",
    "    return await model.allgather(weights[model.rank])\n",
    "\n",
    "dist = Dist(ranks)\n",
    "out_weights = await asyncio.gather(*[\n",
    "    mygather(Model(layers=1, batches=1, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "out_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47f0dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f118c2",
   "metadata": {},
   "source": [
    "### Puzzle 3: Weight-Sharded Data Parallel\n",
    "\n",
    "Rajbhandari, S., Rasley, J., Ruwase, O., and He,\n",
    "Y. Zero: Memory optimizations toward training trillion parameter models, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def wsdp(model: Model) -> Model:\n",
    "    # Storage on device.\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "\n",
    "    # Load all the activations\n",
    "    model.activations[0] = model.get_activation([model.rank])\n",
    "\n",
    "    # Load a shard of the weights for every layer. Load in the full weights\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.load_weights(l, model.rank, model.RANKS) \n",
    "\n",
    "    # Forward\n",
    "    for l in range(model.LAYERS):        \n",
    "        weights[l, 0] = await model.allgather(weights[l])\n",
    "        activations[l + 1] = model.forward(l, activations[l], weights[l, 0])\n",
    "        del weights[l, 0]\n",
    "\n",
    "    # Backward\n",
    "    grad_activations[model.LAYERS] = model.loss(activations[model.LAYERS])\n",
    "\n",
    "    for l in range(model.LAYERS - 1, -1, -1):\n",
    "        weights[l, 0] = await model.allgather(weights[l])\n",
    "        grad_weights[l], grad_activations[l] = model.backward(\n",
    "            l, activations[l], grad_activations[l + 1], weights[l, 0]\n",
    "        )\n",
    "        del grad_activations[l + 1], activations[l], weights[l, 0]\n",
    "\n",
    "    # Update\n",
    "    for l in range(model.LAYERS):\n",
    "        grad_weights[l] = await model.allreduce(grad_weights[l], l)\n",
    "        weights[l], opt_states[l] = model.update(l, grad_weights[l], weights[l], opt_states[l])\n",
    "        model.set_final_weight(l, weights[l])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Dist(ranks)\n",
    "out = await asyncio.gather(*[\n",
    "    wsdp(Model(layers=2, batches=ranks, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "draw_group(out[1].final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bdd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8af3c",
   "metadata": {},
   "source": [
    "## Communication: Scatter-Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b93b0c",
   "metadata": {},
   "source": [
    "Scatter across shards\n",
    "Reduce across batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514dd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_weight = WeightGrad(0, 1, batches={1}, total_batches=4, \n",
    "                         shards={1}, total=4)\n",
    "grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f884be",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_weights = {i: WeightGrad(0, 1, batches={i}, total_batches=4, \n",
    "                         shards={0,1,2,3}, total=4) for i in range(4)}\n",
    "grad_weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scatterreduce(model: Model) -> WeightGrad:\n",
    "    # Allreduce sums together all the weight grads\n",
    "    return await model.scatterreduce(grad_weights[model.rank], 0)\n",
    "\n",
    "dist = Dist(ranks)\n",
    "out = await asyncio.gather(*[\n",
    "    scatterreduce(Model(layers=1, batches=1, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d5f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daebc1db",
   "metadata": {},
   "source": [
    "### Puzzle 4: Fully-Sharded Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b836046",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "async def fsdp(model: Model) -> Model:\n",
    "    # Storage on device.\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "\n",
    "    # Load all the activations\n",
    "    model.activations[0] = model.get_activation([model.rank])\n",
    "\n",
    "    # Load a shard of the weights for every layer. Load in the full weights\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.load_weights(l, model.rank, model.RANKS) \n",
    "\n",
    "    # Forward\n",
    "    for l in range(model.LAYERS):        \n",
    "        weights[l, 0] = await model.allgather(weights[l])\n",
    "        activations[l + 1] = model.forward(l, activations[l], weights[l, 0])\n",
    "        del weights[l, 0]\n",
    "\n",
    "    # Backward\n",
    "    grad_activations[model.LAYERS] = model.loss(activations[model.LAYERS])\n",
    "\n",
    "    for l in range(model.LAYERS - 1, -1, -1):\n",
    "        weights[l, 0] = await model.allgather(weights[l])\n",
    "        grad_weights[l], grad_activations[l] = model.backward(\n",
    "            l, activations[l], grad_activations[l + 1], weights[l, 0]\n",
    "        )\n",
    "        grad_weights[l] = await model.scatterreduce(grad_weights[l], l)\n",
    "        del grad_activations[l + 1], activations[l], weights[l, 0]\n",
    "\n",
    "    # Update\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.update(l, grad_weights[l], weights[l], opt_states[l])\n",
    "        model.set_final_weight(l, weights[l])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a16d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab77d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Dist(ranks)\n",
    "out = await asyncio.gather(*[\n",
    "    fsdp(Model(layers=6, batches=ranks, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "draw_group(out[1].final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced3131",
   "metadata": {},
   "source": [
    "## Communication: Point-to-Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1edb40",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "async def talk(model: Model) -> None:\n",
    "    if model.rank == 0:\n",
    "        await model.pass_to(1, \"extra cheese\")\n",
    "        val = await model.receive()\n",
    "        print(val)\n",
    "    else:\n",
    "        val = await model.receive()\n",
    "        print(val)\n",
    "        val = await model.pass_to(0, \"pizza\")\n",
    "\n",
    "dist = Dist(2)\n",
    "result = await asyncio.gather(*[\n",
    "    talk(Model(layers=1, batches=1, rank=i, dist=dist))\n",
    "    for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660750a1",
   "metadata": {},
   "source": [
    "### Puzzle 5: Pipeline Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42868d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "async def pipeline(model: Model) -> Model:\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "    per_rank = model.LAYERS // model.RANKS\n",
    "    my_layers = list([l + (model.rank * per_rank) for l in range(per_rank)])\n",
    "    for l in my_layers:\n",
    "        weights[l], opt_states[l] = model.load_weights(l)\n",
    "\n",
    "    if model.rank == 0:\n",
    "        activations[0] = model.get_activation(range(model.BATCHES))\n",
    "    else:\n",
    "        activations[my_layers[0]] = await model.receive()\n",
    "\n",
    "    # Forward\n",
    "    for l in my_layers:\n",
    "        activations[l + 1] = model.forward(l, activations[l], weights[l])\n",
    "\n",
    "    # Backward\n",
    "    if model.rank == model.RANKS - 1:\n",
    "        grad_activations[model.LAYERS] = model.loss(\n",
    "            activations[model.LAYERS]\n",
    "        )\n",
    "    else:\n",
    "        await model.pass_to(model.rank + 1, activations[l + 1])\n",
    "        grad_activations[l + 1] = await model.receive()\n",
    "\n",
    "    for l in reversed(my_layers):\n",
    "        grad_weights[l], grad_activations[l] = model.backward(\n",
    "            l, activations[l], grad_activations[l + 1], model.weights[l]\n",
    "        )\n",
    "        del model.grad_activations[l + 1], model.activations[l]\n",
    "\n",
    "    if model.rank != 0:\n",
    "        await model.pass_to(model.rank - 1, grad_activations[l])\n",
    "\n",
    "    # Update\n",
    "    for l in my_layers:\n",
    "        weights[l], opt_states[l] = model.update(l, grad_weights[l], weights[l], opt_states[l])\n",
    "        model.set_final_weight(l, weights[l])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03419a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Dist(ranks)\n",
    "out = await asyncio.gather(*[\n",
    "    pipeline(Model(layers=8, batches=ranks, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "draw_group(out[1].final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f402a1",
   "metadata": {},
   "source": [
    "## Puzzle 6: GPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cb48f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "async def gpipe(model: Model) -> Model:\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "    per_rank = model.LAYERS // model.RANKS\n",
    "    my_layers = list([l + (model.rank * per_rank) for l in range(per_rank)])\n",
    "    for l in my_layers:\n",
    "        weights[l], opt_states[l] = model.load_weights(l)\n",
    "\n",
    "    for mb in range(model.BATCHES):\n",
    "        # Forward\n",
    "        if model.rank == 0:\n",
    "            activations[0, mb] = model.get_activation([mb])\n",
    "        else:\n",
    "            activations[my_layers[0], mb] = await model.receive()\n",
    "\n",
    "        for l in my_layers:\n",
    "            activations[l + 1, mb] = model.forward(l, activations[l, mb], weights[l])\n",
    "        if model.rank != model.RANKS - 1:\n",
    "            await model.pass_to(model.rank + 1, activations[l + 1, mb])\n",
    "\n",
    "    for mb in range(model.BATCHES):\n",
    "        # Backward\n",
    "        if model.rank == model.RANKS - 1:\n",
    "            grad_activations[model.LAYERS, mb] = model.loss(\n",
    "                activations[model.LAYERS, mb]\n",
    "            )\n",
    "        else:\n",
    "            grad_activations[my_layers[-1] + 1, mb] = await model.receive()\n",
    "\n",
    "        for l in reversed(my_layers):\n",
    "            grad_weights[l, mb], grad_activations[l, mb] = model.backward(\n",
    "                l, activations[l, mb], grad_activations[l + 1, mb], weights[l]\n",
    "            )\n",
    "            del grad_activations[l + 1, mb], activations[l, mb]\n",
    "\n",
    "        if model.rank != 0:\n",
    "            await model.pass_to(model.rank - 1, grad_activations[l, mb])\n",
    "\n",
    "    # Update\n",
    "    for l in reversed(my_layers):\n",
    "        for mb in range(model.BATCHES):\n",
    "            if mb != 0:\n",
    "                grad_weights[l] = grad_weights[l] + grad_weights[l, mb]\n",
    "            else: \n",
    "                grad_weights[l] = grad_weights[l, 0]\n",
    "            del grad_weights[l, mb]\n",
    "        weights[l], opt_states[l] = model.update(l, grad_weights[l], weights[l], opt_states[l])\n",
    "        model.set_final_weight(l, weights[l])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ba3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Dist(ranks)\n",
    "out = await asyncio.gather(*[\n",
    "    gpipe(Model(layers=8, batches=ranks, rank=i, dist=dist))\n",
    "    for i in range(ranks)])\n",
    "draw_group(out[1].final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e074a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874b523",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model.check(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f1742",
   "metadata": {},
   "source": [
    "### Puzzle 7: Pipeline + FSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pipeline_fsdp(model: Model) -> Model:\n",
    "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
    "    per_rank = model.LAYERS // (model.RANKS // 4)\n",
    "    my_layers = list([l + ((model.rank % 4)  * per_rank) for l in range(per_rank)])\n",
    "    print(my_layers)\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l, 0], opt_states[l, 0] = model.load_weights(l, model.rank, model.RANKS)\n",
    "\n",
    "\n",
    "    # Forward\n",
    "    for l in range(model.LAYERS):        \n",
    "        if l == my_layers[0]:\n",
    "            if model.rank % 4 == 0:\n",
    "                activations[0] = model.get_activation([model.rank // 4])\n",
    "            else:\n",
    "                activations[l] = await model.receive()\n",
    "    \n",
    "        weights[l] = await model.allgather(weights[l, 0])\n",
    "        if l in my_layers:\n",
    "            activations[l + 1] = model.forward(l, activations[l], weights[l])\n",
    "        del weights[l]\n",
    "        if l == my_layers[-1]:\n",
    "            if model.rank % 4 == 3 :\n",
    "                grad_activations[model.LAYERS] = model.loss(\n",
    "                    activations[model.LAYERS]\n",
    "                )\n",
    "            else:\n",
    "                await model.pass_to(model.rank + 1, activations[l + 1])\n",
    "    # Backward\n",
    "\n",
    "    for l in reversed(range(model.LAYERS)):\n",
    "        if l == my_layers[-1]:\n",
    "            if model.rank % 4 != 3:\n",
    "                grad_activations[l + 1] = await model.receive()\n",
    "    \n",
    "        weights[l] = await model.allgather(weights[l, 0])\n",
    "        if l in my_layers:\n",
    "            grad_weights[l], grad_activations[l] = model.backward(\n",
    "                l, activations[l], grad_activations[l + 1], model.weights[l]\n",
    "            )\n",
    "            del grad_activations[l + 1], activations[l]\n",
    "            grad_weights[l] = await model.scatterreduce(grad_weights[l], l)\n",
    "        else:\n",
    "            grad_weights[l] = await model.scatterreduce(WeightGrad(l, model.LAYERS, frozenset(), model.BATCHES), l)\n",
    "        del weights[l]\n",
    "\n",
    "        if model.rank % 4 != 0 and l == my_layers[0]:\n",
    "            await model.pass_to(model.rank - 1, grad_activations[l])\n",
    "    for l in range(model.LAYERS):\n",
    "        weights[l], opt_states[l] = model.update(l, grad_weights[l], weights[l, 0], opt_states[l, 0])\n",
    "        model.set_final_weight(l, weights[l])\n",
    "\n",
    "    # Update\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe3ffc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dist = Dist(16)\n",
    "out = await asyncio.gather(*[\n",
    "    pipeline_fsdp(Model(layers=8, batches=ranks, rank=i, dist=dist))\n",
    "    for i in range(16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check(out)\n",
    "chalk.set_svg_height(1000)\n",
    "chalk.set_svg_draw_height(1000)\n",
    "\n",
    "draw(out)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
